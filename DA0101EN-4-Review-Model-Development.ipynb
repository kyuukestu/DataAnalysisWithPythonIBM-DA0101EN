{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yhat =  [12809.86873425 18052.11958005 16086.27551287 19362.6822915\n",
      " 13465.15008997 13465.15008997 14120.4314457  14120.4314457\n",
      " -2261.60244743  4291.21110982  4291.21110982  5601.77382127\n",
      "  7567.61788844 12809.86873425  7567.61788844  7567.61788844\n",
      "  7567.61788844 12809.86873425 16741.5568686  -2916.88380316\n",
      "  7567.61788844  4946.49246554 10188.74331134 10188.74331134\n",
      " 10188.74331134 10188.74331134 10844.02466707 10844.02466707\n",
      " 10844.02466707 10844.02466707 14120.4314457  12154.58737852\n",
      " 20017.96364722 12154.58737852  7567.61788844  7567.61788844\n",
      "  7567.61788844  7567.61788844 11499.3060228  11499.3060228\n",
      " 11499.3060228  11499.3060228  11499.3060228  14775.71280142\n",
      " 16086.27551287 16086.27551287 16086.27551287 16086.27551287\n",
      " 20673.24500295  5601.77382127  7567.61788844  7567.61788844\n",
      " 12809.86873425 12809.86873425 11499.3060228  11499.3060228\n",
      " 11499.3060228  12809.86873425 12809.86873425  8222.89924417\n",
      "  -295.75838026  8222.89924417  8222.89924417  8222.89924417\n",
      "  8222.89924417  8222.89924417  8222.89924417  8222.89924417\n",
      "  8222.89924417 10188.74331134 10188.74331134 18052.11958005\n",
      " 18052.11958005 16086.27551287 16086.27551287 17396.83822432\n",
      " 16086.27551287 16741.5568686  10844.02466707 16741.5568686\n",
      " 10844.02466707 16741.5568686  10844.02466707 16741.5568686\n",
      "  5601.77382127 12809.86873425  7567.61788844  7567.61788844\n",
      "  7567.61788844 12809.86873425 14775.71280142 14120.4314457\n",
      " 14120.4314457  14120.4314457  14120.4314457  15430.99415715\n",
      " 15430.99415715  8878.18059989 12154.58737852 12154.58737852\n",
      "  8222.89924417 10844.02466707 11499.3060228  16086.27551287\n",
      " 13465.15008997 11499.3060228  12154.58737852 13465.15008997\n",
      " 17396.83822432  6912.33653272  7567.61788844  7567.61788844\n",
      "  8222.89924417 11499.3060228  11499.3060228   8222.89924417\n",
      "  8222.89924417  8878.18059989  1670.08568692  1670.08568692\n",
      " 10188.74331134 10188.74331134 10188.74331134 10188.74331134\n",
      " 13465.15008997 13465.15008997 12809.86873425 12809.86873425\n",
      " 12809.86873425 12809.86873425 12809.86873425 12809.86873425\n",
      " 10188.74331134 10844.02466707 11499.3060228  11499.3060228\n",
      " 11499.3060228  16741.5568686  16741.5568686  16741.5568686\n",
      "  2325.36704264 10188.74331134  2325.36704264 10188.74331134\n",
      " 10188.74331134  4946.49246554 11499.3060228  13465.15008997\n",
      " 14120.4314457  14120.4314457  14120.4314457  14120.4314457\n",
      " 18052.11958005 18052.11958005 14120.4314457  16086.27551287\n",
      " 17396.83822432 14775.71280142 16086.27551287]\n",
      "         3         2\n",
      "-0.5756 x + 95.52 x - 5071 x + 9.325e+04\n",
      "         3         2\n",
      "-0.5756 x + 95.52 x - 5071 x + 9.325e+04\n",
      "The R-square is:  0.7081148492144188\n",
      "The output of the first four predicted value is:  [ 9785.17540385 14169.40602579 14169.40602579 13357.51146617]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "missing a required argument: 'y_true'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 333\u001b[0m\n\u001b[1;32m    330\u001b[0m Yhat \u001b[38;5;241m=\u001b[39m lm\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe output of the first four predicted value is: \u001b[39m\u001b[38;5;124m'\u001b[39m, Yhat[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m4\u001b[39m])\n\u001b[0;32m--> 333\u001b[0m mean_squared_error \u001b[38;5;241m=\u001b[39m \u001b[43msklearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m mse \u001b[38;5;241m=\u001b[39m mean_squared_error(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m], Yhat)\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe mean square error of price and predicted value is: \u001b[39m\u001b[38;5;124m'\u001b[39m, mse)\n",
      "File \u001b[0;32m~/Desktop/DataAnalysisWithPython/.venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:194\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_sig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m params\u001b[38;5;241m.\u001b[39mapply_defaults()\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# ignore self/cls and positional/keyword markers\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/inspect.py:3267\u001b[0m, in \u001b[0;36mSignature.bind\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   3263\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[1;32m   3264\u001b[0m \u001b[38;5;124;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[1;32m   3265\u001b[0m \u001b[38;5;124;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[1;32m   3266\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/inspect.py:3180\u001b[0m, in \u001b[0;36mSignature._bind\u001b[0;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[1;32m   3178\u001b[0m                 msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing a required\u001b[39m\u001b[38;5;132;01m{argtype}\u001b[39;00m\u001b[38;5;124m argument: \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   3179\u001b[0m                 msg \u001b[38;5;241m=\u001b[39m msg\u001b[38;5;241m.\u001b[39mformat(arg\u001b[38;5;241m=\u001b[39mparam\u001b[38;5;241m.\u001b[39mname, argtype\u001b[38;5;241m=\u001b[39margtype)\n\u001b[0;32m-> 3180\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3182\u001b[0m     \u001b[38;5;66;03m# We have a positional argument to process\u001b[39;00m\n\u001b[1;32m   3183\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: missing a required argument: 'y_true'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.linear_model\n",
    "import sklearn.preprocessing\n",
    "import sklearn.pipeline\n",
    "import sklearn.metrics\n",
    "import skillsnetwork\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv(\"./automobile.csv\", header=0)\n",
    "df = df.replace('?', np.nan)\n",
    "df = df.dropna()\n",
    "df['price'] = df['price'].astype(float)\n",
    "df['peak-rpm'] = df['peak-rpm'].astype(int)\n",
    "df['horsepower'] = df['horsepower'].astype(int)\n",
    "# print(df.dtypes)\n",
    "# print(df.head())\n",
    "\n",
    "#SECTION - Simple Linear Regression \n",
    "\"\"\"NOTE - Simple Linear Regression \n",
    "Simple Linear Regression is a method to help us understand the relationship between two variables:  \n",
    "- the predictor/independent variable (X)\n",
    "- The response/dependent variable (that we want to predict) (Y)\n",
    "\n",
    "The result of Linear Regression is a linear function that predicts the response (dependent) variable as a function of the predictor (independent) variable.\n",
    "\n",
    "Linear Function: \n",
    "Y = a + bX\n",
    "\n",
    "- 'a' refers to the intercept of the regression line, in others words: the value of Y when X is 0\n",
    "- 'b' refers to the slope of the regression line, in others words: the value with which Y changes when X changes by 1 unit\n",
    "\"\"\"\n",
    "# SECTION - LM\n",
    "lm = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "\n",
    "# Create a linear function with 'highway-mpg' as the predictor and 'price' as the response.\n",
    "X = df[['highway-mpg']]\n",
    "Y = df['price']\n",
    "\n",
    "lm.fit(X,Y)\n",
    "\n",
    "Yhat = lm.predict(X)\n",
    "\n",
    "#STUB - LM Print Statements\n",
    "print(\"Yhat = \", Yhat)\n",
    "# print(Yhat[0:5])\n",
    "# print(lm.intercept_)\n",
    "# print(lm.coef_)\n",
    "# print(\"Equation of the line: \\nPrice = \", lm1.coef_, \"* highway-mpg + \", lm1.intercept_)\n",
    "#!SECTION\n",
    "\n",
    "# SECTION - LM1\n",
    "lm1 = sklearn.linear_model.LinearRegression()\n",
    "X = df[['engine-size']]\n",
    "Y = df['price']\n",
    "\n",
    "lm1.fit(X,Y)\n",
    "\n",
    "Yhat1 = lm1.predict(X)\n",
    "\n",
    "#STUB - ML1 Print Statements\n",
    "# print(Yhat[0:5])\n",
    "# print(\"lm1 Y intercept: \", lm1.intercept_)\n",
    "# print(\"lm1 coefficients/slope:\", lm1.coef_)\n",
    "# print(\"Equation of the line: \\nPrice = \", lm1.coef_, \"* Engine Size + \", lm1.intercept_)\n",
    "#!SECTION\n",
    "#!SECTION\n",
    "\n",
    "#SECTION - Multiple Linear Regression\n",
    "\"\"\"NOTE - Multiple Linear Regression\n",
    "What if we want to predict care price using more than one variable?\n",
    "\n",
    "Multiple Linear Regression is very similar to Simple Linear Regression (SLR) but this method is used to explain the relationship between one continuous response (dependent) variable and two or more predictor (independent) variables. Most of the real-world regression models involve multiple predictors.\n",
    "\n",
    "Equation:\n",
    "Yhat = a + b1X1 + b2X2 + b3X3 + b4X4...\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#SECTION - MLR\n",
    "\n",
    "mlr = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "Z = df[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']]\n",
    "mlr.fit(Z, df['price'])\n",
    "Yhat = mlr.predict(Z)\n",
    "\n",
    "#STUB - MLR Example Print Statements\n",
    "# print(mlr.intercept_)\n",
    "# print(mlr.coef_)\n",
    "# print(\"Equation of the line: \\nPrice = \", mlr.coef_[0], \"* horsepower + \", mlr.coef_[1], \"* curb-weight + \", mlr.coef_[2], \"* engine-size + \", mlr.coef_[3], \"* highway-mpg + \", mlr.intercept_)\n",
    "#!SECTION\n",
    "\n",
    "#SECTION - MLR2\n",
    "\n",
    "mlr2 = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "Z = df[['normalized-losses', 'highway-mpg']]\n",
    "mlr2.fit(Z, df['price'])\n",
    "Yhat = mlr2.predict(Z)\n",
    "#STUB - MLR2 Print Statements\n",
    "# print(mlr2.intercept_)\n",
    "# print(mlr2.coef_)\n",
    "# print(\"Equation of the live given by: \\nPrice = \", mlr2.coef_[0], \"* normalized-losses + \", mlr2.coef_[1], \"* highway-mpg + \", mlr2.intercept_)\n",
    "#!SECTION\n",
    "#!SECTION\n",
    "\n",
    "#SECTION - Model Evaluation Using Visualization\n",
    "#SECTION - Regression Plot\n",
    "\"\"\"NOTE - Regression Plot\n",
    "When it comes to simple linear regression, an excellent way to visualize the fit of our model is by using regression plots.\n",
    "\n",
    "This plot will show a combination of scattered data points (a scatter-plot), as well as the fitted linear regression line going through the data. This will give us a reasonable estimate of the relationship between the two variables, the strength of the correlation, as well as the direction (positive or negative correlation).\n",
    "\"\"\"\n",
    "\n",
    "# Let's visualize highway-ppg as the potential predictor variable of price:\n",
    "\n",
    "width = 12\n",
    "height = 10\n",
    "plt.figure(figsize=(width, height))\n",
    "sns.regplot(x=\"highway-mpg\", y=\"price\", data=df)\n",
    "plt.ylim(0,)\n",
    "plt.savefig(\"./Plots/RegPlot/highway-mpgVprice\")\n",
    "plt.clf()\n",
    "\n",
    "# One thing to keep in mind when looking at a regression plot is to pay attention to howe scattered the data points are around the regression line. This will give you a good indication of the variance of the data and whether a linear model would be the best fit or not. If the data is too far off from the line, this linear model might not be the best model for this data.\n",
    "\n",
    "\n",
    "# Let's compare this plot to the regression plot of \"peak-rpm\"\n",
    "\n",
    "plt.figure(figsize=(width, height))\n",
    "sns.regplot(x=\"peak-rpm\", y=\"price\", data=df)\n",
    "plt.ylim(0,)\n",
    "plt.savefig(\"./Plots/RegPlot/peak-rpmVprice\")\n",
    "plt.clf()\n",
    "\n",
    "#STUB - Correlation Print: Highway-mpg, peak-rpm, price\n",
    "# print(df[['highway-mpg', 'peak-rpm', 'price']].corr())\n",
    "\n",
    "#!SECTION\n",
    "#SECTION - Residual Plot\n",
    "\"\"\"NOTE - Residual Plot\n",
    "A good way to visualize the variance of the data is to use a residual plot.\n",
    "\n",
    "What is a residual?\n",
    "\n",
    "The difference between the observed value (y) and the predicted value (Yhat) is called the residual (e). When we look at a regression plot, the residual is the distance from the data point to the fitted regression line.\n",
    "\n",
    "So what is a residual plot?\n",
    "\n",
    "A residual plot is a graph that shows the residuals on the vertical y-axis and the independent variable on the horizontal x-axis.\n",
    "\n",
    "What do we pay attention to when looking at a residual plot?\n",
    "\n",
    "We look at the spread of the residuals:\n",
    "- If the points in a residual plot are randomly spread out around the x-axis, then a linear model is appropriate for the data.\n",
    "\n",
    "Why is that? Randomly spread out residuals means that the variance is constant, and thus the linear model a good fit for this data.\n",
    "\"\"\"\n",
    "\n",
    "width = 12\n",
    "height = 10\n",
    "plt.figure(figsize=(width, height))\n",
    "sns.residplot(x=df['highway-mpg'], y=df['price'])\n",
    "plt.savefig(\"./Plots/ResidPlot/highway-mpgVprice\")\n",
    "plt.clf()\n",
    "\n",
    "# From this residual plot, we can see that the residuals are not randomly spread around the x-axis, leading us to believe that maybe a non-linear model is more appropriate for this data.\n",
    "#!SECTION\n",
    "\n",
    "#SECTION - Visualizing Multiple Linear Regression\n",
    "\"\"\"NOTE - Multiple Linear Regression\n",
    "How do we visualize a model for Multiple Linear Regression? This gets a bit more complicated because you can't visualize it with regression or residual plot.\n",
    "\n",
    "One way to look at the fit of the model is by looking at the distribution plot. We can look at the distribution of the fitted values that result from the model and compare it to the distribution of the actual values.\n",
    "\n",
    "First, let's make a prediction:\n",
    "\"\"\"\n",
    "#TODO - Fix this:\n",
    "# sklearn.set_config(transform_output=\"pandas\")\n",
    "# Z = df[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']]\n",
    "\n",
    "# Y_hat = lm.predict(df[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\n",
    "# plt.figure(figsize=(width, height))\n",
    "\n",
    "\n",
    "# ax1 = sns.displot(df['price'], hist=False, color=\"r\", label=\"Actual Value\")\n",
    "# sns.displot(Y_hat, hist=False, color=\"b\", label=\"Fitted Values\", ax=ax1)\n",
    "\n",
    "# plt.title('Actual vs Fitted Values for Price')\n",
    "# plt.xlabel('Price (in dollars)')\n",
    "# plt.ylabel('Proportion of Cars')\n",
    "\n",
    "# plt.show()\n",
    "# plt.close()\n",
    "\n",
    "#!SECTION\n",
    "\n",
    "#SECTION - Polynomial Regression and Pipelines\n",
    "\"\"\"NOTE - Polynomial Regression\n",
    "\n",
    "Polynomial regression is a particular case of the general linear regression model or multiple linear regression models.\n",
    "\n",
    "We get non-linear relationships by squaring or setting higher-order terms of the predictor variables.\n",
    "\n",
    "There are different orders of polynomial regression:\n",
    "- Quadratic 2nd Order\n",
    "Yhat = a + b1X + b2X^2\n",
    "\n",
    "- Cubic 3rd Order\n",
    "Yhat = a + b1X + b2X^2 + b3X^3\n",
    "\n",
    "- Higher-Order\n",
    "Y = a + b1X + b2X^2 + b3X^3 + ... + bnX^n\n",
    "\n",
    "We saw earlier that a linear model did not provide the best fit while using \"highway-mpg\" as the predictor variable. Let's see if we can try fitting a polynomial model to the data instead.\n",
    "\n",
    "We will use the following function to plot the data:\n",
    "\"\"\"\n",
    "\n",
    "def PlotPolly(model, independent_variable, dependent_variable, Name):\n",
    "    x_new = np.linspace(15, 55, 100)\n",
    "    y_new = model(x_new)\n",
    "\n",
    "    plt.plot(independent_variable, dependent_variable, '.', x_new, y_new, '-')\n",
    "    plt.title('Polynomial Fit with Matplotlib for Price ~ Length')\n",
    "    ax = plt.gca()\n",
    "    ax.set_facecolor((0.898, 0.898, 0.898))\n",
    "    fig = plt.gcf()\n",
    "    plt.xlabel(Name)\n",
    "    plt.ylabel('Price of Cars')\n",
    "    \n",
    "    plt.savefig('./Plots/PolyReg/Polynomial Fit with Matplotlib for Price ~ Length' + Name)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "x = df['highway-mpg']\n",
    "y = df['price']\n",
    "\n",
    "# Here we use a polynomial of the 3rd order (cubic)\n",
    "f = np.polyfit(x, y, 3)\n",
    "p = np.poly1d(f)\n",
    "print(p)\n",
    "#STUB – 3rd Order Function Call\n",
    "# PlotPolly(p, x, y, 'highway-mpg')\n",
    "\n",
    "np.polyfit(x, y, 3)\n",
    "\n",
    "f1 = np.polyfit(x, y, 11)\n",
    "p1 = np.poly1d(f1)\n",
    "print(p)\n",
    "#STUB - 11th Order Function Call\n",
    "# PlotPolly(p1, x, y, 'highway-mpg 11th Order')\n",
    "\n",
    "\n",
    "#We can perform a polynomial transform on multiple features. First, we import the module: Polynomial Features.\n",
    "\n",
    "PolynomialFeatures = sklearn.preprocessing.PolynomialFeatures\n",
    "\n",
    "pr = PolynomialFeatures(degree=2)\n",
    "# print(pr)\n",
    "\n",
    "Z_pr = pr.fit_transform(Z)\n",
    "# print(Z.shape)\n",
    "# print(Z_pr.shape)\n",
    "\n",
    "#SECTION - Pipelines\n",
    "\n",
    "#Data Pipelines simplify the steps of processing the data. We use the module Pipeline to create a pipeline. We also use StandardScaler as a step in our pipeline.\n",
    "\n",
    "StandardScaler = sklearn.preprocessing.StandardScaler\n",
    "Pipeline = sklearn.pipeline.Pipeline\n",
    "LinearRegression = sklearn.linear_model.LinearRegression\n",
    "\n",
    "Input = [('scale', StandardScaler()), ('polynomial', PolynomialFeatures(include_bias=False)), ('model', LinearRegression())]\n",
    "\n",
    "pipe = Pipeline(Input)\n",
    "# print(pipe)\n",
    "\n",
    "Z = Z.astype(float)\n",
    "pipe.fit(Z, y)\n",
    "\n",
    "ypipe = pipe.predict(Z)\n",
    "# print(ypipe[0:4])\n",
    "\n",
    "Input = [('scale',StandardScaler()), ('model', LinearRegression())]\n",
    "\n",
    "pipe = Pipeline(Input)\n",
    "pipe.fit(Z, y)\n",
    "ypipe = pipe.predict(Z)\n",
    "# print(ypipe[0:10]\n",
    "\n",
    "#!SECTION\n",
    "#!SECTION\n",
    "#!SECTION\n",
    "\n",
    "#SECTION - Measures for In-Sample Evaluation\n",
    "\"\"\"NOTE  – Measures for In-Sample Evaluation\n",
    "\n",
    "When evaluating our models, not only do we want to visualize the results, but we also want a quantitative measure to determine how accurate the model is.\n",
    "\n",
    "Two very important measures that are often used in Statistics to determine the accuracy of a model are:\n",
    "- R^2/R-squared\n",
    "- Mean Squared Error (MSE)\n",
    "\n",
    "R-Squared\n",
    "\n",
    "Also known as the coefficient of determination, is a measure to indicate how close the data is to the fitted regression line.\n",
    "\n",
    "The value of the R-squared is the percentage of variation of the response variable (y) that is explained by a linear model.\n",
    "\n",
    "Mean Squared Error (MSE)\n",
    "\n",
    "The Mean Squared Error measures the average of the squares of errors. That is, the difference between actual value (y) and the estimated value (ŷ).\n",
    "\"\"\"\n",
    "\n",
    "#SECTION - Model 1: Simple Linear Regression\n",
    "\n",
    "#Let's calculate the R^2:\n",
    "lm.fit(X, Y)\n",
    "# Find the R^2\n",
    "print('The R-square is: ', lm.score(X, Y))\n",
    "\n",
    "Yhat = lm.predict(X)\n",
    "print('The output of the first four predicted value is: ', Yhat[0:4])\n",
    "\n",
    "mean_squared_error = sklearn.metrics.mean_squared_error()\n",
    "\n",
    "mse = mean_squared_error(df['price'], Yhat)\n",
    "\n",
    "print('The mean square error of price and predicted value is: ', mse)\n",
    "#!SECTION\n",
    "\n",
    "\n",
    "#SECTION - Model 2: Multiple Linear Regression\n",
    "\n",
    "# fit the model\n",
    "lm.fit(Z, df['price'])\n",
    "# Find the R^2\n",
    "print('The R-square is: ', lm.score(Z, df['price']))\n",
    "\n",
    "Y_predict_multifit = lm.predict(Z)\n",
    "\n",
    "print('The mean square error of price and predicted value using multifit is: ', mean_squared_error(df['price'], Y_predict_multifit))\n",
    "\n",
    "#!SECTION\n",
    "\n",
    "\n",
    "#SECTION - Model 3: Polynomial Fit\n",
    "\n",
    "\n",
    "r2_score = sklearn.metrics.r2_score\n",
    "r_squared = r2_score(y, p(x))\n",
    "print('The R-square value is: ', r_squared)\n",
    "\n",
    "#!SECTION\n",
    "\n",
    "#SECTION - MSE\n",
    "\n",
    "mean_squared_error(df['price'], p(x))\n",
    "\n",
    "#!SECTION\n",
    "\n",
    "#SECTION - Prediction and Decision Making\n",
    "\n",
    "new_input = np.arange(1, 100, 1).reshape(-1, 1)\n",
    "\n",
    "lm.fit(X, Y)\n",
    "\n",
    "yhat = lm.predict(new_input)\n",
    "yhat[0:5]\n",
    "\n",
    "plt.plot(new_input, yhat)\n",
    "plt.show()\n",
    "#!SECTION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
